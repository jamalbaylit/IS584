{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d6e8988",
   "metadata": {},
   "source": [
    "# Transformer Architecture Questions\n",
    "\n",
    "Based on the content from the notebook, here are answers to the questions about transformer architecture:\n",
    "\n",
    "## 1. Why do we implement PositionalEncoding in part 1?\n",
    "\n",
    "Positional Encoding is implemented because transformers don't have any inherent understanding of sequence order since they process tokens in parallel rather than sequentially. As explained in the notebook:\n",
    "\n",
    "> We have Positional Encoding, which we require for the Transformer model to understand and relate the relative position of input and output tokens or embeddings.\n",
    "\n",
    "The notebook implements positional encoding using sine and cosine functions of different frequencies:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6982e0",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "PE(pos,2i) = sin(pos/10000^(2i/d_model))\n",
    "PE(pos,2i+1) = cos(pos/10000^(2i/d_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705dbfb8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This allows the model to inject information about token positions without losing the benefits of parallelization.\n",
    "\n",
    "## 2. How do we feed Query, Key, and Value to the multi-head attention blocks?\n",
    "\n",
    "In the notebook's `MultiHeadAttention` class implementation, Query, Key, and Value are processed as follows:\n",
    "\n",
    "1. Input tensors are linearly projected to create query, key, and value representations:\n",
    "   ```python\n",
    "   query = self.split_heads(self.query_linear(query), batch_size)\n",
    "   key = self.split_heads(self.key_linear(key), batch_size)\n",
    "   value = self.split_heads(self.value_linear(value), batch_size) \n",
    "   ```\n",
    "\n",
    "2. These projections are split across multiple attention heads using the `split_heads` method\n",
    "\n",
    "3. Attention scores are computed between query and key:\n",
    "   ```python\n",
    "   attention_weights = self.compute_attention(query, key, mask)\n",
    "   ```\n",
    "\n",
    "4. The attention weights are applied to the values:\n",
    "   ```python\n",
    "   output = torch.matmul(attention_weights, value)\n",
    "   ```\n",
    "\n",
    "5. Results from all heads are concatenated and projected to the output dimension\n",
    "\n",
    "## 3. What is the purpose of the mask (self_attention_mask) defined in Part 9?\n",
    "\n",
    "The `self_attention_mask` in Part 9 is a causal mask that creates a triangular attention pattern. Its purpose is to prevent the decoder from \"cheating\" by looking at future tokens when making predictions. The notebook explains:\n",
    "\n",
    "> The triangular mask defined here is the causal mask that prohibits the decoder from observing the \"future\" or cheating. For the first element in the sequence, the decoder can only observe the first element; for the second, the second and the first; and for the nth element, the decoder can only observe elements (tokens) up to the nth element.\n",
    "\n",
    "This mask is implemented as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_mask = (1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)).bool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5ac3a4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Why do we use split heads in the attention mechanism?\n",
    "\n",
    "Split heads are used in the attention mechanism to allow the model to focus on different representational subspaces simultaneously. The notebook implements this in the `MultiHeadAttention` class:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(self, x, batch_size):\n",
    "    # Split the sequence embeddings in x across the attention heads\n",
    "    x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "    return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa4a17",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The multi-head approach allows different heads to attend to different parts of the input, creating a more powerful attention mechanism. As described in the notebook:\n",
    "\n",
    "> Multi-head attention calculated as:\n",
    "> MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O\n",
    "> where head_i = Attention(Q W^Q_i, K W^K_i, V W^V_i)\n",
    "\n",
    "## 5. What is the difference between self-attention and cross-attention?\n",
    "\n",
    "Based on the notebook:\n",
    "\n",
    "- **Self-attention**: Involves computing attention where query, key, and value all come from the same sequence. Used in both encoders and decoders to allow tokens to attend to other tokens in the same sequence.\n",
    "\n",
    "- **Cross-attention**: Used in the decoder layers of encoder-decoder transformers to attend to encoder outputs. The notebook states:\n",
    "  > The second attention mechanism in the Decoder architecture is Encoder-Decoder attention, or cross-attention layer. The keys and values come from the output of the Encoder stack while queries come from the first self-attention layer of the Decoder stack. With this cross-attention, decoder can attend over all positions in the input.\n",
    "\n",
    "## 6. Where exactly is the cross-attention mask applied in the vanilla transformer architecture?\n",
    "\n",
    "In the vanilla transformer architecture, the cross-attention mask is applied in the decoder layers, specifically in the cross-attention mechanism. From the notebook's `DecoderLayer` implementation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f43b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, causal_mask, encoder_output, cross_mask):\n",
    "    # Multi-head self-attention\n",
    "    self_attn_output = self.self_attn(x, x, x, causal_mask)\n",
    "    x = self.norm1(x + self.dropout(self_attn_output))\n",
    "    # Cross-attention - note the cross_mask being applied here\n",
    "    cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, cross_mask)\n",
    "    x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "    ff_output = self.feed_forward(x)\n",
    "    x = self.norm3(x + self.dropout(ff_output))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0722d7ae",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The cross-attention mask is passed to the cross-attention mechanism to indicate which encoder positions each decoder position should be allowed to attend to.\n",
    "\n",
    "## 7. Which of these (Q, K, and V) is supplied from the Encoder to the cross-attention in the Encoder-Decoder transformer? And which from the decoder's attention?\n",
    "\n",
    "In the Encoder-Decoder transformer's cross-attention mechanism:\n",
    "\n",
    "- **From the Encoder**: Key (K) and Value (V) come from the encoder's output\n",
    "- **From the Decoder**: Query (Q) comes from the decoder's self-attention layer output\n",
    "\n",
    "This is implemented in the `DecoderLayer` class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908002b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, cross_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4387c19e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Where `x` (from the decoder) provides the queries, and `encoder_output` provides both keys and values.\n",
    "\n",
    "## 8. Why are we shifting outputs to the right (in the vanilla Transformer architecture)?\n",
    "\n",
    "The outputs are shifted to the right in the vanilla Transformer architecture to create the target sequence for autoregressive training. In the notebook, this is demonstrated in Part 12:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d20c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sequence = input_sequence.roll(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7033ae6a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This shift creates a training scenario where the model learns to predict the next token in a sequence given the previous tokens, enabling autoregressive generation during inference.\n",
    "\n",
    "## 9. What can we do to improve the training process in Parts 6, 9, and 12?\n",
    "\n",
    "Based on the notebook, several improvements could be made to the training processes:\n",
    "\n",
    "- **Use real data**: Replace random sequences with actual task-specific data\n",
    "- **Hyperparameter tuning**: Optimize model dimensions, learning rates, and dropout rates\n",
    "- **Implement learning rate scheduling**: Utilize warmup and decay strategies\n",
    "- **Add regularization techniques**: Layer normalization and weight tying\n",
    "- **Use larger models**: Increase depth and width for more complex tasks\n",
    "- **Implement more sophisticated optimization**: Use specialized optimizers beyond Adam\n",
    "- **Employ better initialization strategies**: For faster convergence\n",
    "- **Add gradient clipping**: To handle exploding gradients\n",
    "- **Implement early stopping**: To prevent overfitting\n",
    "\n",
    "## 10. What are some possible use-case scenarios for the different transformer types?\n",
    "\n",
    "The notebook outlines specific use cases for each transformer variant:\n",
    "\n",
    "**Encoder-only Transformer**:\n",
    "- Text classification tasks\n",
    "- Named entity recognition\n",
    "- Document classification\n",
    "- Sentiment analysis\n",
    "- BERT is mentioned as a prominent example\n",
    "\n",
    "**Decoder-only Transformer**:\n",
    "- Text generation\n",
    "- Story writing\n",
    "- Code completion\n",
    "- Chat completion\n",
    "- GPT-3 is mentioned as a notable example\n",
    "\n",
    "**Encoder-Decoder Transformer**:\n",
    "- Machine translation\n",
    "- Text summarization\n",
    "- Question answering\n",
    "- Data-to-text generation\n",
    "- T5 (Text-to-Text Transfer Transformer) is mentioned as an example\n",
    "\n",
    "Similar code found with 2 license types"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
